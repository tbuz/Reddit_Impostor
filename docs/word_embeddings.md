# Word Embeddings

In Natural Language Processing (NLP), the term "Word Embedding" refers to the concept of representing the words of a dictionary as vectors. The vectors commonly consist of floating-point numbers such that words with similar meanings are close in vector space.

This representation not only allows finding similar words but also allows doing arithmetic operations on these words. If one were to calculate "king" - "man" + "woman", the closest vector in a well-trained model would be the one representing the word "queen".

## Bag of Words

TODO

## CountVectorizer

TODO

## Word2Vec

TODO

## Sources

- https://en.wikipedia.org/wiki/Word_embedding
- https://de.wikipedia.org/wiki/Worteinbettung
- https://www.youtube.com/watch?v=gQddtTdmG_8
